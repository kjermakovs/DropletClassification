{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using MobileNetV2 and XGBoost\n",
    "\n",
    "This notebook implements an image classification pipeline using:\n",
    "1. **Feature Extraction**: MobileNetV2 pre-trained on ImageNet for deep feature extraction\n",
    "2. **Classification**: XGBoost for efficient multi-class classification\n",
    "3. **Visualization**: UMAP/t-SNE for embedding space visualization\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load images from dataset directory structure\n",
    "2. Extract deep features using MobileNetV2 (with caching for efficiency)\n",
    "3. Train XGBoost classifier on extracted features\n",
    "4. Evaluate model performance with confusion matrix\n",
    "5. Visualize feature embeddings in 3D space (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, max_per_class=None):\n",
    "    \"\"\"\n",
    "    Load images and labels from a directory structure where each subdirectory represents a class.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to dataset root directory containing class subdirectories\n",
    "        max_per_class (int, optional): Maximum number of images to load per class. \n",
    "                                       If None, loads all images. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (images, labels) where images is a list of PIL Images and labels is a list of class names\n",
    "    \"\"\"\n",
    "    images, labels = [], []\n",
    "    \n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        \n",
    "        # Skip if not a directory\n",
    "        if not Path(class_path).is_dir():\n",
    "            continue\n",
    "        \n",
    "        fnames = list(os.listdir(class_path))\n",
    "        \n",
    "        # Shuffle filenames for random sampling (if limiting samples per class)\n",
    "        if max_per_class and max_per_class < 100000:\n",
    "            random.Random(0).shuffle(fnames)\n",
    "            \n",
    "        for idx, image_name in tqdm(enumerate(fnames, start=1), desc=f\"Loading {class_name}\"):\n",
    "            # Skip hidden files\n",
    "            if image_name.startswith(\".\"):\n",
    "                continue\n",
    "                \n",
    "            # Stop if reached max samples for this class\n",
    "            if max_per_class is not None and idx > max_per_class:\n",
    "                break\n",
    "                    \n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            images.append(image)\n",
    "            labels.append(class_name)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def extract_features(image, is_batch=False):\n",
    "    \"\"\"\n",
    "    Extract deep features from images using MobileNetV2 pre-trained on ImageNet.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor(s)\n",
    "        is_batch (bool): Whether input is a batch of images. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Extracted feature vectors (1280-dimensional for MobileNetV2)\n",
    "    \"\"\"\n",
    "    # Add batch dimension if single image\n",
    "    if is_batch:\n",
    "        batch_img_tensor = image\n",
    "    else:\n",
    "        batch_img_tensor = image.unsqueeze(0)\n",
    "\n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_img_tensor = batch_img_tensor.to(device)\n",
    "\n",
    "    # Load pre-trained MobileNetV2 and set to evaluation mode\n",
    "    model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Remove classification layer to extract features from penultimate layer\n",
    "    model.classifier[1] = torch.nn.Identity()\n",
    "\n",
    "    # Extract features without gradient computation\n",
    "    with torch.no_grad():\n",
    "        features = model(batch_img_tensor)\n",
    "\n",
    "    # Flatten and convert to numpy\n",
    "    if is_batch:\n",
    "        features_flattened = torch.flatten(features, start_dim=1).detach().cpu().numpy()\n",
    "    else:\n",
    "        features_flattened = torch.flatten(features, start_dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "    return features_flattened\n",
    "\n",
    "\n",
    "def get_features_batched(images, adjust_size=True):\n",
    "    \"\"\"\n",
    "    Extract features from a batch of images efficiently.\n",
    "    \n",
    "    Args:\n",
    "        images (list): List of PIL Images\n",
    "        adjust_size (bool): Whether to apply preprocessing transforms. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Feature matrix of shape (n_images, 1280)\n",
    "    \"\"\"\n",
    "    # Define standard ImageNet preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Apply preprocessing if needed\n",
    "    if adjust_size:\n",
    "        images = [preprocess(img) for img in images]\n",
    "\n",
    "    # Stack into batch and extract features\n",
    "    batch = torch.stack(images)\n",
    "    features = extract_features(batch, is_batch=True)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class XGBWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class for XGBoost to handle multi-class classification with label encoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, epochs):\n",
    "        \"\"\"\n",
    "        Initialize XGBoost wrapper.\n",
    "        \n",
    "        Args:\n",
    "            params (dict): XGBoost parameters (max_depth, eta, objective, etc.)\n",
    "            epochs (int): Number of boosting rounds\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.epochs = epochs\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train XGBoost model on feature matrix X with labels y.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Feature matrix\n",
    "            y (array-like): Class labels (strings)\n",
    "        \"\"\"\n",
    "        # Encode string labels to integers\n",
    "        labels = self.label_encoder.fit_transform(y)\n",
    "        self.params[\"num_class\"] = len(self.label_encoder.classes_)\n",
    "        dtrain = xgb.DMatrix(X, label=labels)\n",
    "\n",
    "        # Train the model\n",
    "        self.model = xgb.train(self.params, dtrain, self.epochs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for feature matrix X.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Feature matrix\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted class labels (strings)\n",
    "        \"\"\"\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        y_pred = self.model.predict(dtest)\n",
    "        y_pred = y_pred.argmax(axis=1)\n",
    "        y_pred = self.label_encoder.inverse_transform(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Feature Extraction\n",
    "\n",
    "This section loads the dataset and extracts features using MobileNetV2. Features are cached to disk to avoid recomputing them in subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/Users/klavs/Desktop/code-refractored/3\"  # Path to dataset root directory\n",
    "max_per_class = 100000  # Maximum samples per class (higher = better if data quality is good)\n",
    "batch_size = 500  # Batch size for feature extraction (adjust based on GPU memory)\n",
    "dimred = True  # Whether to perform dimensionality reduction visualization\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "images, labels = load_dataset(dataset_path, max_per_class=max_per_class)\n",
    "print(f\"Loaded {len(images)} images across {len(set(labels))} classes\")\n",
    "\n",
    "# Feature caching saves computation time on subsequent runs\n",
    "feat_cache_fname = f\"{dataset_path}/feat_cache_{max_per_class}_mobilenetv2.pkl\"\n",
    "\n",
    "if Path(feat_cache_fname).exists():\n",
    "    # Load pre-computed features from cache\n",
    "    print(f\"Loading features from cache: {feat_cache_fname}\")\n",
    "    \n",
    "    with open(feat_cache_fname, \"rb\") as fp:\n",
    "        X = pkl.loads(fp.read())\n",
    "    \n",
    "    # Validate that cache matches current dataset\n",
    "    if len(images) != len(X):\n",
    "        print(f\"WARNING: Cache size mismatch! Images: {len(images)}, Features: {len(X)}\")\n",
    "        print(\"Re-extracting features...\")\n",
    "        Path(feat_cache_fname).unlink()  # Delete invalid cache\n",
    "        \n",
    "        # Re-extract features (will be handled below)\n",
    "        feats = []\n",
    "        for batch_off in tqdm(range(0, len(images), batch_size), desc=\"Extracting features\"):\n",
    "            this_batch = images[batch_off : batch_off + batch_size]\n",
    "            this_batch_feats = get_features_batched(this_batch)\n",
    "            feats.append(this_batch_feats)\n",
    "        X = np.concatenate(feats, axis=0)\n",
    "        \n",
    "        # Save to cache\n",
    "        with open(feat_cache_fname, \"wb\") as fp:\n",
    "            fp.write(pkl.dumps(X))\n",
    "        print(f\"Features saved to cache: {feat_cache_fname}\")\n",
    "    else:\n",
    "        print(f\"Successfully loaded {X.shape[0]} feature vectors of dimension {X.shape[1]}\")\n",
    "\n",
    "else:\n",
    "    # Extract features in batches (first time or after cache deletion)\n",
    "    print(\"No cache found. Extracting features...\")\n",
    "    feats = []\n",
    "    for batch_off in tqdm(range(0, len(images), batch_size), desc=\"Extracting features\"):\n",
    "        this_batch = images[batch_off : batch_off + batch_size]\n",
    "        this_batch_feats = get_features_batched(this_batch)\n",
    "        feats.append(this_batch_feats)\n",
    "    X = np.concatenate(feats, axis=0)\n",
    "    \n",
    "    # Save features to cache for future use\n",
    "    with open(feat_cache_fname, \"wb\") as fp:\n",
    "        fp.write(pkl.dumps(X))\n",
    "    print(f\"Features saved to cache: {feat_cache_fname}\")\n",
    "    print(f\"Extracted {X.shape[0]} feature vectors of dimension {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "Train an XGBoost classifier on the extracted features and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting dataset into train/test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size=0.05, random_state=44\n",
    ")\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(\"\\nTraining XGBoost classifier...\")\n",
    "\n",
    "# XGBoost hyperparameters\n",
    "params = {\n",
    "    \"max_depth\": 3,          # Maximum tree depth (controls model complexity)\n",
    "    \"eta\": 0.3,              # Learning rate (step size shrinkage)\n",
    "    \"objective\": \"multi:softprob\"  # Multi-class classification with probability output\n",
    "}\n",
    "\n",
    "# Train model\n",
    "clf = XGBWrapper(params, epochs=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save trained model to disk\n",
    "model_save_path = f\"{dataset_path}/xgb_trained_mobilenetv2_max{max_per_class}.pkl\"\n",
    "with open(model_save_path, \"wb\") as fp:\n",
    "    fp.write(pkl.dumps(clf))\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Get unique classes for metric calculations\n",
    "classes = sorted(list(set(y_test)))\n",
    "n_classes = len(classes)\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f} ({int(accuracy * len(y_test))} / {len(y_test)} correct)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Space Visualization (Optional)\n",
    "\n",
    "Visualize the high-dimensional feature embeddings in 3D using dimensionality reduction techniques (t-SNE and UMAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dimred:\n",
    "    print(\"Performing dimensionality reduction for visualization...\")\n",
    "    unique_categories = list(set(labels))\n",
    "    \n",
    "    # Define colors for each class (extend this list if you have more than 3 classes)\n",
    "    colors = [\"#aa0000\", \"#00aa00\", \"#0000aa\", \"#aaaa00\", \"#aa00aa\", \"#00aaaa\"]\n",
    "    color_map = dict(zip(unique_categories, colors[:len(unique_categories)]))\n",
    "    point_colors = [color_map[cat] for cat in labels]\n",
    "    \n",
    "    print(f\"Class colors: {color_map}\")\n",
    "    \n",
    "    print(\"Running t-SNE...\")\n",
    "    tsne = TSNE(n_components=3, random_state=42, verbose=1)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    \n",
    "    # Create 3D scatter plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], c=point_colors, alpha=0.6)\n",
    "    ax.set_title(\"t-SNE Projection of Feature Space\")\n",
    "    ax.set_xlabel(\"t-SNE Component 1\")\n",
    "    ax.set_ylabel(\"t-SNE Component 2\")\n",
    "    ax.set_zlabel(\"t-SNE Component 3\")\n",
    "    \n",
    "    # Create legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color_map[cat], label=cat) for cat in unique_categories]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"tsne_visualization.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"t-SNE visualization saved as 'tsne_visualization.png'\")\n",
    "    print(\"Running UMAP...\")\n",
    "    um = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=3, random_state=42, verbose=True)\n",
    "    X_umap = um.fit_transform(X)\n",
    "    \n",
    "    # Create 3D scatter plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    scatter = ax.scatter(X_umap[:, 0], X_umap[:, 1], X_umap[:, 2], c=point_colors, alpha=0.6)\n",
    "    ax.set_title(\"UMAP Projection of Feature Space\")\n",
    "    ax.set_xlabel(\"UMAP Component 1\")\n",
    "    ax.set_ylabel(\"UMAP Component 2\")\n",
    "    ax.set_zlabel(\"UMAP Component 3\")\n",
    "    \n",
    "    # Create legend\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"umap_visualization.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(\"UMAP visualization saved as 'umap_visualization.png'\")\n",
    "    \n",
    "    print(\"\\nVisualization complete!\")\n",
    "else:\n",
    "    print(\"Dimensionality reduction visualization skipped (dimred=False)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droplet_classification_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
